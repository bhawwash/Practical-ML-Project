#Sample Class
y$class_sample = Complete[sampleInd]
y$class_train = y$class_sample[-testidx]
y$class_test = y$class_sample[testidx]
#Sample from demographic info
y$Pat_Info_Sample = Pat_Info[sampleInd,c('sex','age','state')]
y$Pat_train = y$Pat_Info_Sample[-testidx,]
y$Pat_test = y$Pat_Info_Sample[testidx,]
#Sample from cond/med
y$dat_sample = dat[sampleInd,]
y$dat_train = y$dat_sample[-testidx,]
y$dat_test = y$dat_sample[testidx,]
#Remove columns that are empty in the samples
remove_id=which(((colSums(y$dat_sample)>0)&(colSums(y$dat_train)>0)&(colSums(y$dat_test)>0))==F)
y$dat_sample = y$dat_sample[,-remove_id]
y$dat_train = y$dat_train[,-remove_id]
y$dat_test = y$dat_test[,-remove_id]
return (y)
}
PopulateBinarizedSamples<-function(testidx,sampleInd)
{
y=list()
#Sample Class
y$class_sample = Complete[sampleInd]
y$class_train = y$class_sample[-testidx]
y$class_test = y$class_sample[testidx]
options(na.action="na.pass")
#Sample from demographic info
y$Pat_Info_Sample = cbind(model.matrix(~sex-1,Pat_Info[sampleInd,]),
model.matrix(~state-1,Pat_Info[sampleInd,]),
model.matrix(~age-1,Pat_Info[sampleInd,]))
options('na.action'='na.omit')
y$Pat_train = y$Pat_Info_Sample[-testidx,]
y$Pat_test = y$Pat_Info_Sample[testidx,]
#Sample from cond/med
y$dat_sample = dat[sampleInd,]
y$dat_train = y$dat_sample[-testidx,]
y$dat_test = y$dat_sample[testidx,]
#Remove columns that are empty_bin in the samples
remove_id=which(((colSums(y$dat_sample)>0)&(colSums(y$dat_train)>0)&(colSums(y$dat_test)>0))==F)
y$dat_sample = y$dat_sample[,-remove_id]
y$dat_train = y$dat_train[,-remove_id]
y$dat_test = y$dat_test[,-remove_id]
return (y)
}
CrossValidate<-function(K,type,learn)
{
result = list()
totalAUC = 0;
for(ii in 1:K)
{
print(ii)
testidx <- which((1:SizeSample%%10)+1 == ii)
switch(type,
binary={
#Binary
y = PopulateBinarizedSamples(testidx,sampleInd)
tmp = as.data.frame(cbind(y$class_train,y$Pat_train))
tmp = ConvertToFactors(tmp)
colnames(tmp)[1]='label'
FeatureRemove =apply(summary(tmp),2,function(x){
sum((str_trim(unlist(str_split(x,pattern=":")))[c(1,3)] %in% c(0,1))*1)!=2})
tmp = tmp[,!FeatureRemove]
tmp_test = ConvertToFactors(as.data.frame(y$Pat_test))
#First remove features removed from train
tmp_test = tmp_test[,!FeatureRemove[2:length(FeatureRemove)]]
#Second, set all feature with single value to NA
FeatureRemove2 =apply(summary(tmp_test),2,function(x){
sum((str_trim(unlist(str_split(x,pattern=":")))[c(1,3)] %in% c(0,1))*1)!=2})
tmp_test = tmp_test[,!FeatureRemove2]
tmp = tmp[,c(T,!FeatureRemove2)]
},
history={
#Using only history info
y = PopulateSamples(testidx,sampleInd)
tmp = ConvertToFactors(as.data.frame(cbind(y$class_train,y$dat_train)))
colnames(tmp)[1]<-'label'
tmp_test = ConvertToFactors(as.data.frame(y$dat_test))
},
all={
#using both demographic and history
y = PopulateBinarizedSamples(testidx,sampleInd)
tmp = as.data.frame(cbind(y$class_train,y$Pat_train))
tmp = ConvertToFactors(tmp)
colnames(tmp)[1]='label'
FeatureRemove =apply(summary(tmp),2,function(x){
sum((str_trim(unlist(str_split(x,pattern=":")))[c(1,3)] %in% c(0,1))*1)!=2})
tmp = tmp[,!FeatureRemove]
tmp_test = ConvertToFactors(as.data.frame(y$Pat_test))
#First remove features removed from train
tmp_test = tmp_test[,!FeatureRemove[2:length(FeatureRemove)]]
#Second, set all feature with single value to NA
FeatureRemove2 =apply(summary(tmp_test),2,function(x){
sum((str_trim(unlist(str_split(x,pattern=":")))[c(1,3)] %in% c(0,1))*1)!=2})
tmp_test = tmp_test[,!FeatureRemove2]
tmp = tmp[,c(T,!FeatureRemove2)]
#Add History
tmp = cbind(tmp,ConvertToFactors(as.data.frame(y$dat_train)))
tmp_test = cbind(tmp_test,ConvertToFactors(as.data.frame(y$dat_test)))
}
)
switch(learn,
SVM = {model = svm(formula=label~.,data=tmp,kernel="linear",scale=F)},
RandomForest = {model = randomForest(x=tmp[,2:dim(tmp)[2]],y=tmp[,1])},
Logistic = {model = glm(formula=label~.,dat=tmp,family="binomial")},
NaiveBayes = {model = naiveBayes(formula = label~.,data=tmp)}
)
switch(learn,
Logistic = {
pred =  round(predict(model, newdata=tmp_test, type='response'))
result[[ii]]=EvalBinaryModel(pred,y$class_test)
#totalAUC=totalAUC+EvalBinaryModel(pred,y$class_test)
},
{
pred <-predict(model,tmp_test)
result[[ii]]=EvalBinaryModel(as.numeric(as.character(pred)),y$class_test)
#totalAUC=totalAUC+ EvalBinaryModel(as.numeric(as.character(pred)),y$class_test)
})
}
#return(totalAUC/K)
return (result)
}
out_svm = CrossValidate(10,'all','SVM')
PlotResults<-function(result_list,exp_name)
{
result = as.data.frame(x=NULL)
model_names = c("SVM","Naive Bayes","Random Forests","Logistic Regression")
j=1;
auc=matrix(nrow=4,ncol=1,data=0)
for(a in result_list)
{
for(i in 1:K)
{
x=as.numeric(slot(a[[i]][[2]],"x.values")[[1]])
y=as.numeric(slot(a[[i]][[2]],"y.values")[[1]])
auc[j] = auc[j]+a[[i]][[1]]
result = rbind(result,cbind(x,y,i,j))
}
j =j +1;
}
auc = auc/K
result = as.data.frame(result)
result$j = model_names[result$j]
windows();ggplot(as.data.frame(result),aes(x,y,group=factor(i),color=factor(i)))+
geom_line()+ggtitle(exp_name)+stat_smooth
xlab("False Positive Rate")+ylab("True Positive Rate")+
scale_color_discrete(name="Run")+
facet_grid(~j,labeller=function(v,value){return(paste(model_names[value],'AUC:',auc[value]))})
}
EvalBinaryModel <-function(pred,true_label)
{
require(ROCR)
pred <- prediction(pred, true_label)
perf <- performance(pred, "tpr", "fpr")
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
#windows();plot(perf,main=paste("AUC is ",auc))
return (list(auc,perf))
}
EvalModel <- function(prediction,test)
{
class_names = colnames(prediction,test)
total = 0;
for(catg in class_names)
{
score = prediction[,catg]
actual_class <- test$mode == catg
pred <- prediction(score, actual_class)
perf <- performance(pred, "tpr", "fpr")
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
total=total+auc;
}
return (total/length(class_names))
}
ConvertToFactors <-function(dat)
{
for (i in 1:(dim(dat)[2])){
dat[is.na(dat[,i]),i]=0
dat[,i]=as.factor(dat[,i])}
return (dat)
}
out_svm = CrossValidate(10,'all','SVM')
SizeSample = 10000
sampleInd = stratified(Complete,SizeSample/2)
out_svm = CrossValidate(10,'all','SVM')
PlotResults(list(CrossValidate(10,'all','SVM'),
CrossValidate(10,'all','NaiveBayes'),
CrossValidate(10,'all','RandomForest'),
CrossValidate(10,'all','Logistic')),"All Features")
auc
result
PlotResults<-function(result_list,exp_name)
{
K=10
result = as.data.frame(x=NULL)
model_names = c("SVM","Naive Bayes","Random Forests","Logistic Regression")
j=1;
auc=matrix(nrow=4,ncol=1,data=0)
for(a in result_list)
{
for(i in 1:K)
{
x=as.numeric(slot(a[[i]][[2]],"x.values")[[1]])
y=as.numeric(slot(a[[i]][[2]],"y.values")[[1]])
auc[j] = auc[j]+a[[i]][[1]]
result = rbind(result,cbind(x,y,i,j))
}
j =j +1;
}
auc = auc/K
result = as.data.frame(result)
result$j = model_names[result$j]
windows();ggplot(as.data.frame(result),aes(x,y,group=factor(i),color=factor(i)))+
geom_line()+ggtitle(exp_name)+stat_smooth
xlab("False Positive Rate")+ylab("True Positive Rate")+
scale_color_discrete(name="Run")+
facet_grid(~j,labeller=function(v,value){return(paste(model_names[value],'AUC:',auc[value]))})
}
PlotResults(list(CrossValidate(10,'all','SVM'),
CrossValidate(10,'all','NaiveBayes'),
CrossValidate(10,'all','RandomForest'),
CrossValidate(10,'all','Logistic')),"All Features")
library(ggplot2)
PlotResults(list(CrossValidate(10,'all','SVM'),
CrossValidate(10,'all','NaiveBayes'),
CrossValidate(10,'all','RandomForest'),
CrossValidate(10,'all','Logistic')),"All Features")
result_list = list(CrossValidate(10,'all','SVM'),
CrossValidate(10,'all','NaiveBayes'),
CrossValidate(10,'all','RandomForest'),
CrossValidate(10,'all','Logistic'))
PlotResults<-function(result_list,exp_name)
{
K=10
result = as.data.frame(x=NULL)
model_names = c("SVM","Naive Bayes","Random Forests","Logistic Regression")
j=1;
auc=matrix(nrow=4,ncol=1,data=0)
for(a in result_list)
{
for(i in 1:K)
{
x=as.numeric(slot(a[[i]][[2]],"x.values")[[1]])
y=as.numeric(slot(a[[i]][[2]],"y.values")[[1]])
auc[j] = auc[j]+a[[i]][[1]]
result = rbind(result,cbind(x,y,i,j))
}
j =j +1;
}
auc = auc/K
result = as.data.frame(result)
result$j = model_names[result$j]
windows();ggplot(as.data.frame(result),aes(x,y,group=factor(i),color=factor(i)))+
geom_line()+ggtitle(exp_name)+
xlab("False Positive Rate")+ylab("True Positive Rate")+
scale_color_discrete(name="Run")+
facet_grid(~j,labeller=function(v,value){return(paste(model_names[value],'AUC:',auc[value]))})
}
PlotResults(result_list,"All ")
sampleInd = sample(1:dim(dat)[1],SizeSample)
PlotResults(list(CrossValidate(10,'all','SVM'),
CrossValidate(10,'all','NaiveBayes'),
CrossValidate(10,'all','RandomForest'),
CrossValidate(10,'all','Logistic')),"All Features (Random)")
PlotResults(list(CrossValidate(10,'history','SVM'),
CrossValidate(10,'history','NaiveBayes'),
CrossValidate(10,'history','RandomForest'),
CrossValidate(10,'history','Logistic')),"History (Random)")
PlotResults(list(CrossValidate(10,'binary','SVM'),
CrossValidate(10,'binary','NaiveBayes'),
CrossValidate(10,'binary','RandomForest'),
CrossValidate(10,'binary','Logistic')),"Demographic (Random)")
#Stratified sample to get equal number of samples from each class
sampleInd = stratified(Complete,SizeSample/2)
PlotResults(list(CrossValidate(10,'all','SVM'),
CrossValidate(10,'all','NaiveBayes'),
CrossValidate(10,'all','RandomForest'),
CrossValidate(10,'all','Logistic')),"All Features (Stratified)")
PlotResults(list(CrossValidate(10,'history','SVM'),
CrossValidate(10,'history','NaiveBayes'),
CrossValidate(10,'history','RandomForest'),
CrossValidate(10,'history','Logistic')),"History (Stratified)")
PlotResults(list(CrossValidate(10,'binary','SVM'),
CrossValidate(10,'binary','NaiveBayes'),
CrossValidate(10,'binary','RandomForest'),
CrossValidate(10,'binary','Logistic')),"Demographic (Stratified)")
setwd("D:/Shared Folder/ml")
install.library(caret)
install.packages("caret")
library(caret)
library(caret)
library(parallel)
library(doParallel)
install.packages("doParallel")
library(caret)
library(parallel)
library(doParallel)
registerDoParallel(makeCluster(detectCores()))
detectCores()
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-1]
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format="%d/%m/%Y")
training = training[,c(159,1:158)]
d = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
training = cbind(training[,-c(2,5,6)],predict(d,newdata=training))
d=sapply(training[,-1],function(x){as.numeric(as.character(x))})
training[,2:165] = as.data.frame(d)
PercNull<- colSums(1*is.na(training))/dim(training)[1]
training = training[,PercNull<0.9]
set.seed(1234)
IDX = createDataPartition(y=training$classe,p=0.7,list=F)
train_set = training[IDX,]
valid_set = training[-IDX,]
nz=nearZeroVar(train_set,saveMetrics=T)
train_set = train_set[,!nz$nzv]
m = train(classe~.,data=train_set,method="lda")
m
m = train(classe~.,data=train_set,method="glm")
m = train(classe~.,data=train_set,method="gbm")
m
set.seed(1234)
IDX = createDataPartition(y=training$classe,p=0.7,list=F)
train_set = training[IDX,]
valid_set = training[-IDX,]
d = preProcess(train_set[,-1],method=c("center","scale"))
train_set[,2:65] = predict(d,newdata=train_set[,-1])
nz=nearZeroVar(train_set,saveMetrics=T)
train_set = train_set[,!nz$nzv]
valid_set[,2:65] = predict(d,newdata=valid_set[,-1])
valid_set = valid_set[,!nz$nzv]
m = train(classe~.,data=train_set,method="gbm")
m
rf_accuracy<- predict(m, valid_set)
print(confusionMatrix(rf_accuracy, valid_set$classe))
m_lda = train(classe~.,data=train_set,method="lda")
m_gbm=m
m_lda
m_rf = train(classe~.,data=train_set,method="rf")
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-1]
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format="%d/%m/%Y")
training = training[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
training = cbind(training[,-c(2,5,6)],predict(d,newdata=training))
training[,2:165] =as.data.frame(sapply(training[,-1],function(x){as.numeric(as.character(x))}))
PercNull<- colSums(1*is.na(training))/dim(training)[1]
training = training[,PercNull<0.9]
testing = testing[,-1]
testing$cvtd_timestamp=as.Date(testing$cvtd_timestamp,format="%d/%m/%Y")
testing = testing[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=testing)
testing = cbind(testing[,-c(2,5,6)],predict(d,newdata=testing))
testing[,2:165] =as.data.frame(sapply(testing[,-1],function(x){as.numeric(as.character(x))}))
testing = testing[,PercNull<0.9]
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-1]
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format="%d/%m/%Y")
training = training[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
training = cbind(training[,-c(2,5,6)],predict(d,newdata=training))
training[,2:165] =as.data.frame(sapply(training[,-1],function(x){as.numeric(as.character(x))}))
PercNull<- colSums(1*is.na(training))/dim(training)[1]
training = training[,PercNull<0.9]
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-1]
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format="%d/%m/%Y")
training = training[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
training = cbind(training[,-c(2,5,6)],predict(d,newdata=training))
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-1]
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format="%d/%m/%Y")
training = training[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-1]
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format="%d/%m/%Y")
training = training[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
training = cbind(training[,-c(2,5,6)],predict(dummy))
training[,2:165] =as.data.frame(sapply(training[,-1],function(x){as.numeric(as.character(x))}))
PercNull<- colSums(1*is.na(training))/dim(training)[1]
training = training[,PercNull<0.9]
testing = testing[,-1]
testing$cvtd_timestamp=as.Date(testing$cvtd_timestamp,format="%d/%m/%Y")
testing = testing[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=testing)
testing = cbind(testing[,-c(2,5,6)],predict(dummy,newdata=testing))
testing[,2:165] =as.data.frame(sapply(testing[,-1],function(x){as.numeric(as.character(x))}))
testing = testing[,PercNull<0.9]
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-1]
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format="%d/%m/%Y")
training = training[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
training = cbind(training[,-c(2,5,6)],predict(dummy,newdata=training))
training[,2:165] =as.data.frame(sapply(training[,-1],function(x){as.numeric(as.character(x))}))
PercNull<- colSums(1*is.na(training))/dim(training)[1]
training = training[,PercNull<0.9]
testing = testing[,-1]
testing$cvtd_timestamp=as.Date(testing$cvtd_timestamp,format="%d/%m/%Y")
testing = testing[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=testing)
testing = cbind(testing[,-c(2,5,6)],predict(dummy,newdata=testing))
testing[,2:165] =as.data.frame(sapply(testing[,-1],function(x){as.numeric(as.character(x))}))
testing = testing[,PercNull<0.9]
testing = read.csv("pml-testing.csv")
testing = testing[,-1]
testing$cvtd_timestamp=as.Date(testing$cvtd_timestamp,format="%d/%m/%Y")
testing = testing[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=testing)
testing
str(testing)
testing = read.csv("pml-testing.csv")
testing = testing[,-1]
testing$cvtd_timestamp=as.Date(testing$cvtd_timestamp,format="%d/%m/%Y")
testing = testing[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=testing)
testing$user_name
testing$new_window
testing$cvtd_timestamp
dummyVars(classe~user_name+new_window+cvtd_timestamp,data=testing)
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=testing)
classe~user_name+new_window+cvtd_timestamp
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-1]
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format="%d/%m/%Y")
training = training[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=testing)
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
head(testing)
testing = testing[,-1]
testing$cvtd_timestamp=as.Date(testing$cvtd_timestamp,format="%d/%m/%Y")
testing = testing[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=testing)
head(testing)
testing = read.csv("pml-testing.csv")
head(testing)
testing = testing[,-1]
testing$cvtd_timestamp=as.Date(testing$cvtd_timestamp,format="%d/%m/%Y")
testing = testing[,c(159,1:158)]
dummy = dummyVars(problem_id~user_name+new_window+cvtd_timestamp,data=testing)
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-1]
training$cvtd_timestamp=as.Date(training$cvtd_timestamp,format="%d/%m/%Y")
training = training[,c(159,1:158)]
dummy = dummyVars(classe~user_name+new_window+cvtd_timestamp,data=training)
training = cbind(training[,-c(2,5,6)],predict(dummy,newdata=training))
training[,2:165] =as.data.frame(sapply(training[,-1],function(x){as.numeric(as.character(x))}))
PercNull<- colSums(1*is.na(training))/dim(training)[1]
training = training[,PercNull<0.9]
testing = testing[,-1]
testing$cvtd_timestamp=as.Date(testing$cvtd_timestamp,format="%d/%m/%Y")
testing = testing[,c(159,1:158)]
dummy = dummyVars(problem_id~user_name+new_window+cvtd_timestamp,data=testing)
testing = cbind(testing[,-c(2,5,6)],predict(dummy,newdata=testing))
testing[,2:165] =as.data.frame(sapply(testing[,-1],function(x){as.numeric(as.character(x))}))
testing = testing[,PercNull<0.9]
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
head(training)
training = training[,-c(1:7)]
training = training[,c(153,1:152)]
training[,2:153] =as.data.frame(sapply(training[,-1],function(x){as.numeric(as.character(x))}))
PercNull<- colSums(1*is.na(training))/dim(training)[1]
training = training[,PercNull<0.9]
testing = testing[,-c(1:7)]
testing = testing[,c(153,1:152)])
testing[,2:153] =as.data.frame(sapply(testing[,-1],function(x){as.numeric(as.character(x))}))
testing = testing[,PercNull<0.9]
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
training = training[,-c(1:7)]
training = training[,c(153,1:152)]
training[,2:153] =as.data.frame(sapply(training[,-1],function(x){as.numeric(as.character(x))}))
PercNull<- colSums(1*is.na(training))/dim(training)[1]
training = training[,PercNull<0.9]
testing = testing[,-c(1:7)]
testing = testing[,c(153,1:152)]
testing[,2:153] =as.data.frame(sapply(testing[,-1],function(x){as.numeric(as.character(x))}))
testing = testing[,PercNull<0.9]
set.seed(1234)
IDX = createDataPartition(y=training$classe,p=0.7,list=F)
train_set = training[IDX,]
valid_set = training[-IDX,]
d = preProcess(train_set[,-1],method=c("center","scale"))
train_set[,2:53] = predict(d,newdata=train_set[,-1])
nz=nearZeroVar(train_set,saveMetrics=T)
train_set = train_set[,!nz$nzv]
nz
set.seed(1234)
IDX = createDataPartition(y=training$classe,p=0.7,list=F)
train_set = training[IDX,]
valid_set = training[-IDX,]
d = preProcess(train_set[,-1],method=c("center","scale"))
train_set[,2:53] = predict(d,newdata=train_set[,-1])
valid_set[,2:53] = predict(d,newdata=valid_set[,-1])
set.seed(123)
m_lda = train(classe~.,data=train_set,method="lda")
